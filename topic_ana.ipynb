{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "from jieba import posseg\n",
    "from jieba import analyse\n",
    "import pyLDAvis \n",
    "import pyLDAvis.sklearn\n",
    "import ipywidgets\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import AuthorTopicModel\n",
    "import matplotlib.pyplot as plt \n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from pylab import mpl\n",
    "import nltk\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.cluster import KMeans\n",
    "# import codecs\n",
    "# from textrank4zh import TextRank4Keyword, TextRank4Sentence\n",
    "jieba.add_word('pm2.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data1 = pd.read_csv(open('大气.csv', encoding='utf-8')).astype(str).head(1000)\n",
    "data1.columns=['作者', '文章名称', '关键词', '摘要', '发表位置', '时间', '单位或其他', '下载量', '被引量'] \n",
    "data2 = pd.read_csv(open('水环境.csv', encoding='utf-8')).astype(str).head(1000)\n",
    "data2.columns=['作者', '文章名称', '关键词', '摘要', '发表位置', '时间', '单位或其他', '下载量', '被引量'] \n",
    "data3 = pd.read_csv(open('土环境.csv', encoding='utf-8')).astype(str).head(1000) \n",
    "data3.columns=['作者', '文章名称', '关键词', '摘要', '发表位置', '时间', '单位或其他', '下载量', '被引量'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.to_csv('1.csv', encoding=\"utf_8_sig\")\n",
    "data2.to_csv('2.csv', encoding=\"utf_8_sig\")\n",
    "data3.to_csv('3.csv', encoding=\"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data1,data2,data3],axis=0, ignore_index=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(data[data['作者'] == 'nan'].index| data[data['关键词'] == 'nan'].index)\n",
    "data = data.drop_duplicates().reset_index()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理单一文本分词\n",
    "with open(r'hit_stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    stop = [word.strip('\\n') for word in f.readlines()]\n",
    "def chinese_word_cut(text):\n",
    "    text = text.replace('nan', '')\n",
    "    text = text.replace('\\n', '')\n",
    "    l = []\n",
    "    pos = ['n', 'nz', 'v', 'vd', 'vn', 'l', 'a', 'd', 'x', 'eng', 'ns']  # 定义选取的词性\n",
    "    text = text.lower()\n",
    "    seg = posseg.cut(text)  # 分词\n",
    "    for i in seg:\n",
    "        if i.word not in stop and i.flag in pos:  # 去停用词 + 词性筛选\n",
    "            l.append(i.word)\n",
    "    return ' '.join(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_word_cut(text):\n",
    "    text = text.replace('\\n', '')\n",
    "    text = text.split('/')\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['key_words'] = data.关键词.apply(key_word_cut)\n",
    "data['content_cutted'] = data.摘要.apply(chinese_word_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_process(text):\n",
    "    text = text.replace('\\n', '')\n",
    "    text = text.replace('\\t', '')\n",
    "    text = text[:4]\n",
    "    return text\n",
    "data['date'] = data.时间.apply(date_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date'] = pd.to_datetime(data['date'])\n",
    "data = data.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['2015']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = Counter((' '.join((data.key_words).tolist()).split()))\n",
    "Counter(cnt).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按年份统计发文数量\n",
    "count = []\n",
    "years = []\n",
    "for year in range(2014, 2021):\n",
    "    years.append(year)\n",
    "    cnt = list(set(data[str(year)].count().values))[0]\n",
    "    count.append(int(cnt))\n",
    "plt.bar(years,count,facecolor='g',edgecolor='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计词频，选取了前20个关键字  作为主题\n",
    "cnt = Counter((' '.join((data.key_words).tolist()).lower().split()))\n",
    "topic = dict(Counter(cnt).most_common(20))\n",
    "# 以关键词第一个为主题进行绘制的（后期需要哪方面可以改）\n",
    "plt.rcParams['font.sans-serif'] = ['KaiTi']\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.pie(topic.values(),radius=1,wedgeprops=dict(width=0.4,edgecolor='w'),labels=topic.keys())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义关键字  按年份统计发文数量\n",
    "%matplotlib inline\n",
    "toplist = ['pm2.5',  '空气质量', '大气污染', '雾霾', '细颗粒物', '水污染']\n",
    "topicdict = {}\n",
    "years = []\n",
    "for year in range(2014, 2021):\n",
    "    years.append(year)\n",
    "    cnt = Counter((' '.join((data[str(year)].key_words).tolist()).lower().split()))\n",
    "    cnt\n",
    "    topic = dict(Counter(cnt).most_common())\n",
    "    for word in toplist:\n",
    "        if word not in topicdict.keys():\n",
    "            topicdict[word] = []\n",
    "        try:\n",
    "            topicdict[word].append(topic[word])\n",
    "        except:\n",
    "            topicdict[word].append(0)\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "for word in topicdict.keys():\n",
    "    plt.plot(years,topicdict[word],label=word,linewidth=2.0,linestyle='--', )\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qikan = data['发表位置'].value_counts()[:10]\n",
    "name = qikan.index.tolist()\n",
    "value = qikan.values\n",
    "plt.bar(name,value ,facecolor='g',edgecolor='r')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据关键字查找 \n",
    "def get_paper(label):\n",
    "    return data[data['key_words'].str.contains(label)]['文章名称']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_paper('空气污染')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 进行textrank分析 同样可以按年份、或者批量的文章来设置 分析关键词\n",
    "keywords = jieba.analyse.textrank(' '.join((data.key_words).tolist()), topK=100, withWeight=True, allowPOS=('n', 'nz', 'v', 'vd', 'vn', 'l', 'a', 'd', 'x', 'eng'))  # TextRank关键词提取，词性筛选\n",
    "# word_split = \" \".join(keywords)\n",
    "# print (word_split)\n",
    "df = pd.DataFrame.from_dict(dict(keywords), orient='index').reset_index()\n",
    "df.columns = ['key_words', 'weights']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 进行textrank分析 同样可以按年份、或者批量的文章来设置 分析摘要\n",
    "keywords = jieba.analyse.textrank(' '.join((data.content_cutted).tolist()), topK=100, withWeight=True, allowPOS=('n', 'nz', 'v', 'vd', 'vn', 'l', 'a', 'd', 'x', 'eng'))  # TextRank关键词提取，词性筛选\n",
    "# word_split = \" \".join(keywords)\n",
    "# print (word_split)\n",
    "df = pd.DataFrame.from_dict(dict(keywords), orient='index').reset_index()\n",
    "df.columns = ['key_words', 'weights']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ' '.join((data.key_words).tolist())\n",
    "wc = WordCloud(font_path=r'msyh.ttf',background_color='white',width=800,height=600,max_font_size=100,  \n",
    "               max_words=100, regexp='(?u)\\\\w+\\\\.\\\\w+|\\\\w\\\\w+',collocations=False )#,min_font_size=10)#,mode='RGBA',colormap='pink')  \n",
    "\n",
    "wc.generate(result)  \n",
    "wc.to_file(r\"key_words.png\") #按照设置的像素宽高度保存绘制好的词云图，比下面程序显示更清晰  \n",
    "  \n",
    "# 4、显示图片  \n",
    "plt.figure(figsize=(16,12))\n",
    "plt.figure(\"词云图\") #指定所绘图名称  \n",
    "plt.imshow(wc)       # 以图片的形式显示词云  \n",
    "plt.axis(\"off\")      #关闭图像坐标系  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ' '.join((data.content_cutted).tolist())\n",
    "wc = WordCloud(font_path=r'msyh.ttf',background_color='white',width=800,height=600,max_font_size=100,  \n",
    "               max_words=100, regexp='(?u)\\\\w+\\\\.\\\\w+|\\\\w\\\\w+',collocations=False )#,min_font_size=10)#,mode='RGBA',colormap='pink')  \n",
    "\n",
    "wc.generate(result)  \n",
    "wc.to_file(r\"key_words.png\") #按照设置的像素宽高度保存绘制好的词云图，比下面程序显示更清晰  \n",
    "  \n",
    "# 4、显示图片  \n",
    "plt.figure(figsize=(16,12))\n",
    "plt.figure(\"词云图\") #指定所绘图名称  \n",
    "plt.imshow(wc)       # 以图片的形式显示词云  \n",
    "plt.axis(\"off\")      #关闭图像坐标系  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    kmeans文本聚类，效果一般，数据相关性比较大 改用文本分类可检索\n",
    "# n_features = 50\n",
    "# tf_vectorizer = CountVectorizer(strip_accents='unicode',\n",
    "#                                 max_features=n_features,\n",
    "#                                 token_pattern='(?u)\\\\b\\\\w+\\\\.\\\\w+\\\\b|(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
    "# tfidfv = TfidfTransformer()\n",
    "# tf = tf_vectorizer.fit_transform(data.key_words)\n",
    "# tfidf = tfidfv.fit_transform(tf)\n",
    "# word = tf_vectorizer.get_feature_names() #获取词袋模型中的所有词语\n",
    "# weight = tfidf.toarray()\n",
    "\n",
    "# inter = {}\n",
    "# for num in range(3, 100, 3):\n",
    "#     num_clusters = num \n",
    "\n",
    "#     km = KMeans(n_clusters=num_clusters, max_iter=1000, precompute_distances=True)\n",
    "\n",
    "#     km.fit(weight)\n",
    "\n",
    "#     clusters = km.labels_.tolist()\n",
    "#     # 样本距其最近的聚类中心的平方距离之和，用来评判分类的准确度，值越小越好\n",
    "#     # k-means的超参数n_clusters可以通过该值来评估\n",
    "#     print(\"inertia: {}\".format(km.inertia_))\n",
    "#     inter[num] = km.inertia_\n",
    "# print(inter)\n",
    "# 聚类越多，值越小   此处还有问题，可尝试采用文本分类  手动标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda根据keywords分析文章主题       同样可根据时间或其他方式分类进行    \n",
    "doclist = data.key_words.values\n",
    "doclist\n",
    "texts = [[word for word in doc.split()] for doc in doclist]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "model_list = []\n",
    "\n",
    "for i in range(3):\n",
    "    lda_gensim = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=3,passes = 10,iterations=1000, random_state=i)\n",
    "    top_topics = lda_gensim.top_topics(corpus)\n",
    "    tc = sum([t[1] for t in top_topics])\n",
    "    model_list.append((lda_gensim, tc))\n",
    "\n",
    "# 模型评估:主题一致性    \n",
    "print(model_list)\n",
    "lda_gensim, tc = max(model_list, key=lambda x: x[1])\n",
    "print('Topic coherence: %.3e' %tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "print(lda_gensim.print_topics(num_topics=10, num_words=5))\n",
    "pyLDAvis.show(pyLDAvis.gensim.prepare(lda_gensim, corpus, dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda根据摘要分析文章主题       同样可根据时间或其他方式分类进行\n",
    "doclist = data.content_cutted.values\n",
    "doclist\n",
    "texts = [[word for word in doc.split()] for doc in doclist]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "model_list = []\n",
    "\n",
    "for i in range(3):\n",
    "    lda_gensim = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=3,passes = 10,iterations=1000, random_state=i)\n",
    "    top_topics = lda_gensim.top_topics(corpus)\n",
    "    tc = sum([t[1] for t in top_topics])\n",
    "    model_list.append((lda_gensim, tc))\n",
    "\n",
    "# 模型评估:主题一致性    \n",
    "print(model_list)\n",
    "lda_gensim, tc = max(model_list, key=lambda x: x[1])\n",
    "print('Topic coherence: %.3e' %tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "print(lda_gensim.print_topics(num_topics=10, num_words=5))\n",
    "pyLDAvis.show(pyLDAvis.gensim.prepare(lda_gensim, corpus, dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_vectorizer = CountVectorizer(strip_accents='unicode',\n",
    "#                                 max_features=n_features,\n",
    "#                                 token_pattern='(?u)\\\\b\\\\w+\\\\.\\\\w+\\\\b|(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
    "# tf = tf_vectorizer.fit_transform(data.key_words)\n",
    "# tf_features_names = tf_vectorizer.get_feature_names()\n",
    "# n_topics = 3\n",
    "# lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=1000,\n",
    "#                                     random_state=0)\n",
    "# lda.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_vectorizer = CountVectorizer(strip_accents='unicode',\n",
    "#                                 max_features=n_features,\n",
    "#                                 token_pattern='(?u)\\\\b\\\\w+\\\\.\\\\w+\\\\b|(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
    "# tf = tf_vectorizer.fit_transform(data.content_cutted)\n",
    "# tf_features_names = tf_vectorizer.get_feature_names()\n",
    "# n_topics = 3\n",
    "# lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=1000,\n",
    "#                                     random_state=0)\n",
    "# lda.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_csv('1.csv', encoding=\"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_complete=data.key_words.values\n",
    "texts = [[word for word in doc.split()] for doc in doc_complete]\n",
    "dictionary_aut = corpora.Dictionary(texts)\n",
    "    # 使用上面的词典，将转换文档列表（语料）变成 DT 矩阵\n",
    "doc_term_matrix = [dictionary_aut.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aut_name = data.作者.tolist()\n",
    "author_name=set()\n",
    "author_list=[]\n",
    "author2doc={}\n",
    "count=0\n",
    "for line in aut_name:\n",
    "    for name in line.split('/'):\n",
    "        if name not in author_name:\n",
    "            author_name.add(name)\n",
    "            author_list=[]\n",
    "            author_list.append(count)\n",
    "            author2doc[name]=author_list\n",
    "        else:\n",
    "            author2doc[name].append(count)\n",
    "        \n",
    "    count = count + 1\n",
    "    \n",
    "author2doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi = dict(zip(dictionary_aut.token2id.values(), dictionary_aut.token2id.keys()))\n",
    "# model = AuthorTopicModel(corpus=doc_term_matrix,author2doc=author2doc,num_topics=3 , id2word=mi, passes=10, iterations=2000, random_state=12)\n",
    "# model.update(doc_term_matrix, author2doc)\n",
    "# author_vecs = [model.get_author_topics(author) for author in model.id2author.values()]\n",
    "# print(author_vecs)\n",
    "# model = AuthorTopicModel(corpus=doc_term_matrix, num_topics=3, id2word=mi, \\\n",
    "#                 author2doc=author2doc, chunksize=2000, passes=1, eval_every=0, \\\n",
    "#                 iterations=1, random_state=1)\n",
    "\n",
    "# 模型选择\n",
    "#如果你觉得模型没有收敛，那么可以采用增量训练,`model.update(corpus, author2doc)`。\n",
    "#通过设置该参数random_state,不同的随机种子,并选择具有最高主题一致性的模型。\n",
    "model_list = []\n",
    "for i in range(1):\n",
    "    model = AuthorTopicModel(corpus=doc_term_matrix, num_topics=3, id2word=mi, \\\n",
    "                    author2doc=author2doc, passes=1, gamma_threshold=1e-10, \\\n",
    "                    eval_every=0, iterations=10, random_state=i)\n",
    "    top_topics = model.top_topics(doc_term_matrix)\n",
    "    tc = sum([t[1] for t in top_topics])\n",
    "    model_list.append((model, tc))\n",
    "\n",
    "# 模型评估:主题一致性    \n",
    "print(model_list)\n",
    "model, tc = max(model_list, key=lambda x: x[1])\n",
    "print('Topic coherence: %.3e' %tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import atmodel\n",
    "# doc2author = atmodel.construct_doc2author(model.corpus, model.author2doc)\n",
    "\n",
    "# # Compute the per-word bound.\n",
    "# # Number of words in corpus.\n",
    "# corpus_words = sum(cnt for document in model.corpus for _, cnt in document)\n",
    "\n",
    "# # Compute bound and divide by number of words.\n",
    "# perwordbound = model.bound(model.corpus, author2doc=model.author2doc, \\\n",
    "#                            doc2author=model.doc2author) / corpus_words\n",
    "# print(perwordbound)\n",
    "\n",
    "# # 话题一致性指标计算 \n",
    "# top_topics = model.top_topics(model.corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义每个主题主要内容\n",
    "topic_labels = ['农药', '环境污染', 'pm2.5']\n",
    "\n",
    "# 查看每个主题下都有哪些词语\n",
    "for topic in model.show_topics(num_topics=3):\n",
    "    print('Label: ' + topic_labels[topic[0]])\n",
    "    words = ''\n",
    "    for word, prob in model.show_topic(topic[0]):\n",
    "        words += word + ' '\n",
    "    print('Words: ' + words)\n",
    "    print()\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "def show_author(name):\n",
    "    print('\\n%s' % name)\n",
    "    print('Docs:', model.author2doc[name])\n",
    "    print('Topics:')\n",
    "    pprint([(topic_labels[topic[0]], topic[1]) for topic in model[name]])\n",
    "\n",
    "# 作者的主要文章有哪些，话题有那个\n",
    "show_author('朱琳')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model['朱琳'])\n",
    "model.get_author_topics('朱琳')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 相似作者推荐\n",
    "# from gensim.similarities import MatrixSimilarity\n",
    "# import numpy as np\n",
    "\n",
    "# # Generate a similarity object for the transformed corpus.\n",
    "# index = MatrixSimilarity(model[list(model.id2author.values())])\n",
    "\n",
    "# # Get similarities to some author.\n",
    "# author_name = '朱琳'\n",
    "# sims = index[model[author_name]]\n",
    "# idxs = np.argsort(sims)[-11:-1]\n",
    "# for idx in idxs.tolist():\n",
    "#     print(model.id2author[idx], sims[idx], model[model.id2author[idx]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 相似作者\n",
    "from gensim import matutils\n",
    "import pandas as pd\n",
    "\n",
    "# Make a list of all the author-topic distributions.\n",
    "author_vecs = [model.get_author_topics(author) for author in model.id2author.values()]\n",
    "\n",
    "def similarity(vec1, vec2):\n",
    "    '''Get similarity between two vectors'''\n",
    "    dist = matutils.hellinger(matutils.sparse2full(vec1, model.num_topics), \\\n",
    "                              matutils.sparse2full(vec2, model.num_topics))\n",
    "    sim = 1.0 / (1.0 + dist)\n",
    "    return sim\n",
    "\n",
    "def get_sims(vec):\n",
    "    '''Get similarity of vector to all authors.'''\n",
    "    sims = [similarity(vec, vec2) for vec2 in author_vecs]\n",
    "    return sims\n",
    "\n",
    "def get_table(name, top_n=10, smallest_author=1):\n",
    "    '''\n",
    "    Get table with similarities, author names, and author sizes.\n",
    "    Return `top_n` authors as a dataframe.\n",
    "\n",
    "    '''\n",
    "\n",
    "    # Get similarities.\n",
    "    sims = get_sims(model.get_author_topics(name))\n",
    "\n",
    "    # Arrange author names, similarities, and author sizes in a list of tuples.\n",
    "    table = []\n",
    "    for elem in enumerate(sims):\n",
    "        author_name = model.id2author[elem[0]]\n",
    "        sim = elem[1]\n",
    "        author_size = len(model.author2doc[author_name])\n",
    "        if author_size >= smallest_author:\n",
    "            table.append((author_name, sim, author_size))\n",
    "\n",
    "    # Make dataframe and retrieve top authors.\n",
    "    df = pd.DataFrame(table, columns=['作者', '相似度', '文档数量'])\n",
    "    df = df.sort_values('相似度', ascending=False)[:top_n]\n",
    "\n",
    "    return df\n",
    "get_table('朱琳', smallest_author=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "smallest_author = 0  # Ignore authors with documents less than this.\n",
    "authors = [model.author2id[a] for a in model.author2id.keys() if len(model.author2doc[a]) >= smallest_author]\n",
    "_ = tsne.fit_transform(model.state.gamma[authors, :])  # Result stored in tsne.embedding_\n",
    "\n",
    "# Tell Bokeh to display plots inside the notebook.\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "from bokeh.models import HoverTool\n",
    "from bokeh.plotting import figure, show, ColumnDataSource\n",
    "\n",
    "x = tsne.embedding_[:, 0]\n",
    "y = tsne.embedding_[:, 1]\n",
    "author_names = [model.id2author[a] for a in authors]\n",
    "\n",
    "# Radius of each point corresponds to the number of documents attributed to that author.\n",
    "scale = 0.1\n",
    "author_sizes = [len(model.author2doc[a]) for a in author_names]\n",
    "radii = [size * scale for size in author_sizes]\n",
    "\n",
    "source = ColumnDataSource(\n",
    "        data=dict(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            author_names=author_names,\n",
    "            author_sizes=author_sizes,\n",
    "            radii=radii,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Add author names and sizes to mouse-over info.\n",
    "hover = HoverTool(\n",
    "        tooltips=[\n",
    "        (\"author\", \"@author_names\"),\n",
    "        (\"size\", \"@author_sizes\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "p = figure(tools=[hover, 'crosshair,pan,wheel_zoom,box_zoom,reset,save,lasso_select'])\n",
    "p.scatter('x', 'y', radius='radii', source=source, fill_alpha=0.6, line_color=None)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['font.sans-serif'] = ['SimHei']\n",
    "ntext = nltk.Text((' '.join(data['content_cutted'].tolist())).split())\n",
    "print(ntext.dispersion_plot(['污染', '大气', '颗粒物', 'pm2.5']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
